{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.8.0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ffca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                           # Python의 random 시드 설정\n",
    "    np.random.seed(seed)                        # Numpy의 random 시드 설정\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU 시드 설정\n",
    "    torch.cuda.manual_seed(seed)                # PyTorch GPU 시드 설정\n",
    "    torch.cuda.manual_seed_all(seed)            # 모든 GPU의 시드를 설정 (멀티 GPU 환경)\n",
    "\n",
    "    # CuDNN의 비결정적 연산을 방지하여 재현성을 높입니다.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "random_state = 42\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':3,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b90a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pdb import set_trace as stx\n",
    "import numbers\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Layer Norm\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Gated-Dconv Feed-Forward Network (GDFN)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q,k,v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "##########################################################################\n",
    "##---------- Restormer -----------------------\n",
    "class Restormer(nn.Module):\n",
    "    def __init__(self,\n",
    "        inp_channels=3,\n",
    "        out_channels=3,\n",
    "        dim = 48,\n",
    "        num_blocks = [4,6,6,8],\n",
    "        num_refinement_blocks = 4,\n",
    "        heads = [1,2,4,8],\n",
    "        ffn_expansion_factor = 2.66,\n",
    "        bias = False,\n",
    "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
    "    ):\n",
    "\n",
    "        super(Restormer, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
    "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
    "        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n",
    "\n",
    "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
    "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
    "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "\n",
    "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
    "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
    "\n",
    "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
    "        self.dual_pixel_task = dual_pixel_task\n",
    "        if self.dual_pixel_task:\n",
    "            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        ###########################\n",
    "\n",
    "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "\n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
    "\n",
    "        inp_enc_level4 = self.down3_4(out_enc_level3)\n",
    "        latent = self.latent(inp_enc_level4)\n",
    "\n",
    "        inp_dec_level3 = self.up4_3(latent)\n",
    "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
    "\n",
    "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
    "\n",
    "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "\n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "\n",
    "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
    "        if self.dual_pixel_task:\n",
    "            out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)\n",
    "            out_dec_level1 = self.output(out_dec_level1)\n",
    "        ###########################\n",
    "        else:\n",
    "            out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
    "\n",
    "\n",
    "        return out_dec_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "restormer_arch = Restormer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa29db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numbers\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "\n",
    "def to_4d(x, h, w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma + 1e-5) * self.weight\n",
    "\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type == 'BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "class DFFN(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "\n",
    "        super(DFFN, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim * ffn_expansion_factor)\n",
    "\n",
    "        self.patch_size = 8\n",
    "\n",
    "        self.dim = dim\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,\n",
    "                                groups=hidden_features * 2, bias=bias)\n",
    "\n",
    "        self.fft = nn.Parameter(torch.ones((hidden_features * 2, 1, 1, self.patch_size, self.patch_size // 2 + 1)))\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x_patch = rearrange(x, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        x_patch_fft = torch.fft.rfft2(x_patch.float())\n",
    "        x_patch_fft = x_patch_fft * self.fft\n",
    "        x_patch = torch.fft.irfft2(x_patch_fft, s=(self.patch_size, self.patch_size))\n",
    "        x = rearrange(x_patch, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
    "                      patch2=self.patch_size)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FSAS(nn.Module):\n",
    "    def __init__(self, dim, bias):\n",
    "        super(FSAS, self).__init__()\n",
    "\n",
    "        self.to_hidden = nn.Conv2d(dim, dim * 6, kernel_size=1, bias=bias)\n",
    "        self.to_hidden_dw = nn.Conv2d(dim * 6, dim * 6, kernel_size=3, stride=1, padding=1, groups=dim * 6, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(dim * 2, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.norm = LayerNorm(dim * 2, LayerNorm_type='WithBias')\n",
    "\n",
    "        self.patch_size = 8\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.to_hidden(x)\n",
    "\n",
    "        q, k, v = self.to_hidden_dw(hidden).chunk(3, dim=1)\n",
    "\n",
    "        q_patch = rearrange(q, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        k_patch = rearrange(k, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        q_fft = torch.fft.rfft2(q_patch.float())\n",
    "        k_fft = torch.fft.rfft2(k_patch.float())\n",
    "\n",
    "        out = q_fft * k_fft\n",
    "        out = torch.fft.irfft2(out, s=(self.patch_size, self.patch_size))\n",
    "        out = rearrange(out, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
    "                        patch2=self.patch_size)\n",
    "\n",
    "        out = self.norm(out)\n",
    "\n",
    "        output = v * out\n",
    "        output = self.project_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False, LayerNorm_type='WithBias', att=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.att = att\n",
    "        if self.att:\n",
    "            self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "            self.attn = FSAS(dim, bias)\n",
    "\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = DFFN(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.att:\n",
    "            x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Fuse(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Fuse, self).__init__()\n",
    "        self.n_feat = n_feat\n",
    "        self.att_channel = TransformerBlock(dim=n_feat * 2)\n",
    "\n",
    "        self.conv = nn.Conv2d(n_feat * 2, n_feat * 2, 1, 1, 0)\n",
    "        self.conv2 = nn.Conv2d(n_feat * 2, n_feat * 2, 1, 1, 0)\n",
    "\n",
    "    def forward(self, enc, dnc):\n",
    "        x = self.conv(torch.cat((enc, dnc), dim=1))\n",
    "        x = self.att_channel(x)\n",
    "        x = self.conv2(x)\n",
    "        e, d = torch.split(x, [self.n_feat, self.n_feat], dim=1)\n",
    "        output = e + d\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n",
    "                                  nn.Conv2d(n_feat, n_feat * 2, 3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                                  nn.Conv2d(n_feat, n_feat // 2, 3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "##---------- FFTformer -----------------------\n",
    "class fftformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_channels=3,\n",
    "                 out_channels=3,\n",
    "                 dim=48,\n",
    "                 num_blocks=[6, 6, 12, 8],\n",
    "                 num_refinement_blocks=4,\n",
    "                 ffn_expansion_factor=3,\n",
    "                 bias=False,\n",
    "                 ):\n",
    "        super(fftformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in\n",
    "            range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2 = Downsample(dim)\n",
    "        self.encoder_level2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3 = Downsample(int(dim * 2 ** 1))\n",
    "        self.encoder_level3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.decoder_level3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.refinement = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_refinement_blocks)])\n",
    "\n",
    "        self.fuse2 = Fuse(dim * 2)\n",
    "        self.fuse1 = Fuse(dim)\n",
    "        self.output = nn.Conv2d(int(dim), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "\n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
    "\n",
    "        out_dec_level3 = self.decoder_level3(out_enc_level3)\n",
    "\n",
    "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "\n",
    "        inp_dec_level2 = self.fuse2(inp_dec_level2, out_enc_level2)\n",
    "\n",
    "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
    "\n",
    "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "\n",
    "        inp_dec_level1 = self.fuse1(inp_dec_level1, out_enc_level1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "\n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "\n",
    "        out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
    "\n",
    "        return out_dec_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "fftformer_arch = fftformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, ToPILImage, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import CenterCrop, Resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import *\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = clean_image_paths  # 이미 리스트로 전달됨\n",
    "        self.noisy_image_paths = noisy_image_paths  # 이미 리스트로 전달됨\n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((224, 224))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "\n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return noisy_clean_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "\n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "\n",
    "        if self.transform:\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "            torch.manual_seed(seed)\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "            torch.manual_seed(seed)  # Re-seed to ensure identical transform\n",
    "            clean_image = self.transform(clean_image)\n",
    "\n",
    "        return noisy_image, clean_image\n",
    "\n",
    "# 각 블러 유형 리스트에 대해 데이터셋 구성 및 데이터 로더 생성 함수\n",
    "def create_dataloader(noisy_image_paths, clean_image_paths, batch_size, transform, test_size=0.2):\n",
    "    # 이미지 파일 경로로부터 ID 기반 딕셔너리 생성\n",
    "    noisy_files_dict = {'_'.join(os.path.basename(x).split('_')[:-1]): x for x in noisy_image_paths}\n",
    "    clean_files_dict = {'_'.join(os.path.basename(x).split('_')[:-1]): x for x in clean_image_paths}\n",
    "\n",
    "    # ID 기준으로 noisy-clean 짝을 맞추기\n",
    "    noisy_clean_pairs = [(noisy_files_dict[id], clean_files_dict[id]) for id in noisy_files_dict if id in clean_files_dict]\n",
    "    if not noisy_clean_pairs:\n",
    "        print(\"No pairs found!\")\n",
    "        return None, None\n",
    "\n",
    "    noisy_files_matched, clean_files_matched = zip(*noisy_clean_pairs)\n",
    "\n",
    "    # 데이터셋 분할 (8:2 비율)\n",
    "    train_noisy, val_noisy, train_clean, val_clean = train_test_split(\n",
    "        noisy_files_matched, clean_files_matched, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # CustomDataset을 사용하여 학습 및 검증 데이터셋 생성\n",
    "    train_dataset = CustomDataset(train_noisy, train_clean, transform=transform)\n",
    "    val_dataset = CustomDataset(val_noisy, val_clean, transform=transform)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, scheduler, scaler, device, patience=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "\n",
    "    best_loss = float('inf')  # 초기값을 매우 큰 수로 설정\n",
    "    best_model = None\n",
    "    epochs_no_improve = 0  # 개선되지 않은 epoch 수를 저장할 변수\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        # Training loop\n",
    "        for noisy_imgs, clean_imgs in tqdm(iter(train_loader), desc=f\"Epoch {epoch}/{CFG['EPOCHS']}\"):\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #With autocast()나 gradscaler를 사용하면 float16으로 계산하는 과정에서 왜곡이 생길 수 있다고 하니, 실험해봐야 함.\n",
    "            with autocast():\n",
    "                outputs = model(noisy_imgs)\n",
    "                l1_loss = criterion(outputs, clean_imgs)\n",
    "\n",
    "            scaler.scale(l1_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss.append(l1_loss.item() * noisy_imgs.size(0))\n",
    "\n",
    "        # Validation step\n",
    "        _val_loss = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss: [{_train_loss:.5f}] Val Loss: [{_val_loss:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "\n",
    "        # Check for improvement\n",
    "        if _val_loss < best_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "            epochs_no_improve = 0  # 개선되었으면 카운트 초기화\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy_imgs, clean_imgs in tqdm(iter(val_loader), desc=\"Validation\"):\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "\n",
    "            pred = model(noisy_imgs)\n",
    "\n",
    "            loss = criterion(pred, clean_imgs)\n",
    "\n",
    "            val_loss.append(loss.item() * noisy_imgs.size(0))\n",
    "\n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "    return _val_loss\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for noisy_imgs in tqdm(test_loader, desc=\"Inference\"):\n",
    "            noisy_imgs = noisy_imgs.to(device).float()\n",
    "            pred = model(noisy_imgs)\n",
    "\n",
    "            preds.append(pred.detach().cpu())\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "\n",
    "    return preds\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_df(model, train_loader, val_loader):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG[\"LEARNING_RATE\"], weight_decay=1e-4)\n",
    "    criterion = nn.L1Loss()\n",
    "    scaler = GradScaler()\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=CFG[\"EPOCHS\"])\n",
    "\n",
    "    infer_model = train(model, optimizer, train_loader, val_loader, scheduler, scaler, device)\n",
    "\n",
    "    return infer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "#데이터셋 경로\n",
    "base_dir = './data/Training'\n",
    "noisy_image_paths = os.path.join(base_dir, 'noisy')\n",
    "clean_image_paths = os.path.join(base_dir, 'clean')\n",
    "\n",
    "clean_files = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "\n",
    "# 각 유형별로 리스트 초기화\n",
    "ff_rf_paths = []\n",
    "rl_ud_paths = []\n",
    "lb_rb_paths = []\n",
    "other_blur_paths = []\n",
    "\n",
    "# 이미지 파일 리스트 생성\n",
    "all_image_paths = glob(os.path.join(noisy_image_paths, '*.jpg'))\n",
    "\n",
    "# 파일 경로 리스트 분류\n",
    "for image_path in all_image_paths:\n",
    "    filename = os.path.basename(image_path)\n",
    "\n",
    "    # 블러 유형 추출 (FI 등)\n",
    "    blur_type = filename.split('_')[-1][:2]\n",
    "\n",
    "    if blur_type in ['FF', 'RF']:\n",
    "        ff_rf_paths.append(image_path)\n",
    "    elif blur_type in ['RL', 'UD']:\n",
    "        rl_ud_paths.append(image_path)\n",
    "    elif blur_type in ['LB', 'RB', 'DU', 'FI']:\n",
    "        lb_rb_paths.append(image_path)\n",
    "    else:\n",
    "        other_blur_paths.append(image_path)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"FF, RF 블러 유형 이미지 파일 경로 리스트:\", len(ff_rf_paths))\n",
    "print(\"RL, UD 블러 유형 이미지 파일 경로 리스트:\", len(rl_ud_paths))\n",
    "print(\"LB, RB 블러 유형 이미지 파일 경로 리스트:\", len(lb_rb_paths))\n",
    "print(\"기타 블러 유형 이미지 파일 경로 리스트:\", len(other_blur_paths))\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "train_transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "#     RandomRotation(degrees=10),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #official github에서 안 써서 쓰면 안됨.\n",
    "])\n",
    "\n",
    "# 예시로 FF, RF, RL, UD, 기타 유형별 데이터 로더 생성\n",
    "ff_rf_train_loader, ff_rf_val_loader = create_dataloader(ff_rf_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "rl_ud_train_loader, rl_ud_val_loader = create_dataloader(rl_ud_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "lb_rb_train_loader, lb_rb_val_loader = create_dataloader(lb_rb_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "other_blur_train_loader, other_blur_val_loader = create_dataloader(other_blur_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "\n",
    "# 예시 출력\n",
    "print(\"FF, RF 블러 유형 학습/검증 데이터 로더:\", len(ff_rf_train_loader), len(ff_rf_val_loader))\n",
    "print(\"RL, UD 블러 유형 학습/검증 데이터 로더:\", len(rl_ud_train_loader), len(rl_ud_val_loader))\n",
    "print(\"LB, RB 블러 유형 학습/검증 데이터 로더:\", len(lb_rb_train_loader), len(lb_rb_val_loader))\n",
    "print(\"기타 블러 유형 학습/검증 데이터 로더:\", len(other_blur_train_loader), len(other_blur_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96387e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_rb_model = restormer_arch.to(device)\n",
    "lb_rb_restormer_weight = torch.load('./single_image_defocus_deblurring.pth')\n",
    "lb_rb_model.load_state_dict(lb_rb_restormer_weight['params'])\n",
    "\n",
    "lb_rb_model = train_df(lb_rb_model, lb_rb_train_loader, lb_rb_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lb_rb_model.state_dict(), 'lb_rb_restormer_3epoch.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
