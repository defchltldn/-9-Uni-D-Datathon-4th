{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3e6e3c89",
   "metadata": {},
   "source": [
    "#        (Image Deblurring)\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123fd37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting opencv-python==4.8.0.74\n",
      "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.8.0.74) (1.24.4)\n",
      "Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.0.74\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python==4.8.0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792c53a",
   "metadata": {},
   "source": [
    "### Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa6cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                           # Python random  \n",
    "    np.random.seed(seed)                        # Numpy random  \n",
    "    torch.manual_seed(seed)                     # PyTorch CPU  \n",
    "    torch.cuda.manual_seed(seed)                # PyTorch GPU  \n",
    "    torch.cuda.manual_seed_all(seed)            #  GPU   ( GPU )\n",
    "    \n",
    "    # CuDNN     .\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "random_state = 42\n",
    "\n",
    "# GPU   \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d6494",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d97675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pdb import set_trace as stx\n",
    "import numbers\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Layer Norm\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Gated-Dconv Feed-Forward Network (GDFN)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q,k,v = qkv.chunk(3, dim=1)   \n",
    "        \n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "##########################################################################\n",
    "##---------- Restormer -----------------------\n",
    "class Restormer(nn.Module):\n",
    "    def __init__(self, \n",
    "        inp_channels=3, \n",
    "        out_channels=3, \n",
    "        dim = 48,\n",
    "        num_blocks = [4,6,6,8], \n",
    "        num_refinement_blocks = 4,\n",
    "        heads = [1,2,4,8],\n",
    "        ffn_expansion_factor = 2.66,\n",
    "        bias = False,\n",
    "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
    "    ):\n",
    "\n",
    "        super(Restormer, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        \n",
    "        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
    "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
    "        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n",
    "        \n",
    "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
    "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
    "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "\n",
    "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
    "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        \n",
    "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
    "        \n",
    "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
    "        self.dual_pixel_task = dual_pixel_task\n",
    "        if self.dual_pixel_task:\n",
    "            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        ###########################\n",
    "            \n",
    "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
    "\n",
    "        inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
    "        latent = self.latent(inp_enc_level4) \n",
    "                        \n",
    "        inp_dec_level3 = self.up4_3(latent)\n",
    "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "        out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
    "\n",
    "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "        out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
    "\n",
    "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "        \n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "\n",
    "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
    "        if self.dual_pixel_task:\n",
    "            out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)\n",
    "            out_dec_level1 = self.output(out_dec_level1)\n",
    "        ###########################\n",
    "        else:\n",
    "            out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
    "\n",
    "\n",
    "        return out_dec_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fftformer_arch = fftformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fb856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numbers\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "\n",
    "def to_4d(x, h, w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma + 1e-5) * self.weight\n",
    "\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type == 'BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "class DFFN(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "\n",
    "        super(DFFN, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim * ffn_expansion_factor)\n",
    "\n",
    "        self.patch_size = 8\n",
    "\n",
    "        self.dim = dim\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,\n",
    "                                groups=hidden_features * 2, bias=bias)\n",
    "\n",
    "        self.fft = nn.Parameter(torch.ones((hidden_features * 2, 1, 1, self.patch_size, self.patch_size // 2 + 1)))\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x_patch = rearrange(x, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        x_patch_fft = torch.fft.rfft2(x_patch.float())\n",
    "        x_patch_fft = x_patch_fft * self.fft\n",
    "        x_patch = torch.fft.irfft2(x_patch_fft, s=(self.patch_size, self.patch_size))\n",
    "        x = rearrange(x_patch, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
    "                      patch2=self.patch_size)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FSAS(nn.Module):\n",
    "    def __init__(self, dim, bias):\n",
    "        super(FSAS, self).__init__()\n",
    "\n",
    "        self.to_hidden = nn.Conv2d(dim, dim * 6, kernel_size=1, bias=bias)\n",
    "        self.to_hidden_dw = nn.Conv2d(dim * 6, dim * 6, kernel_size=3, stride=1, padding=1, groups=dim * 6, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(dim * 2, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.norm = LayerNorm(dim * 2, LayerNorm_type='WithBias')\n",
    "\n",
    "        self.patch_size = 8\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.to_hidden(x)\n",
    "\n",
    "        q, k, v = self.to_hidden_dw(hidden).chunk(3, dim=1)\n",
    "\n",
    "        q_patch = rearrange(q, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        k_patch = rearrange(k, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "                            patch2=self.patch_size)\n",
    "        q_fft = torch.fft.rfft2(q_patch.float())\n",
    "        k_fft = torch.fft.rfft2(k_patch.float())\n",
    "\n",
    "        out = q_fft * k_fft\n",
    "        out = torch.fft.irfft2(out, s=(self.patch_size, self.patch_size))\n",
    "        out = rearrange(out, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
    "                        patch2=self.patch_size)\n",
    "\n",
    "        out = self.norm(out)\n",
    "\n",
    "        output = v * out\n",
    "        output = self.project_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False, LayerNorm_type='WithBias', att=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.att = att\n",
    "        if self.att:\n",
    "            self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "            self.attn = FSAS(dim, bias)\n",
    "\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = DFFN(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.att:\n",
    "            x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Fuse(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Fuse, self).__init__()\n",
    "        self.n_feat = n_feat\n",
    "        self.att_channel = TransformerBlock(dim=n_feat * 2)\n",
    "\n",
    "        self.conv = nn.Conv2d(n_feat * 2, n_feat * 2, 1, 1, 0)\n",
    "        self.conv2 = nn.Conv2d(n_feat * 2, n_feat * 2, 1, 1, 0)\n",
    "\n",
    "    def forward(self, enc, dnc):\n",
    "        x = self.conv(torch.cat((enc, dnc), dim=1))\n",
    "        x = self.att_channel(x)\n",
    "        x = self.conv2(x)\n",
    "        e, d = torch.split(x, [self.n_feat, self.n_feat], dim=1)\n",
    "        output = e + d\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n",
    "                                  nn.Conv2d(n_feat, n_feat * 2, 3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                                  nn.Conv2d(n_feat, n_feat // 2, 3, stride=1, padding=1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "##---------- FFTformer -----------------------\n",
    "class fftformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_channels=3,\n",
    "                 out_channels=3,\n",
    "                 dim=48,\n",
    "                 num_blocks=[6, 6, 12, 8],\n",
    "                 num_refinement_blocks=4,\n",
    "                 ffn_expansion_factor=3,\n",
    "                 bias=False,\n",
    "                 ):\n",
    "        super(fftformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in\n",
    "            range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2 = Downsample(dim)\n",
    "        self.encoder_level2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3 = Downsample(int(dim * 2 ** 1))\n",
    "        self.encoder_level3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.decoder_level3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.refinement = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, att=True) for i in range(num_refinement_blocks)])\n",
    "\n",
    "        self.fuse2 = Fuse(dim * 2)\n",
    "        self.fuse1 = Fuse(dim)\n",
    "        self.output = nn.Conv2d(int(dim), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "\n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
    "\n",
    "        out_dec_level3 = self.decoder_level3(out_enc_level3)\n",
    "\n",
    "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "\n",
    "        inp_dec_level2 = self.fuse2(inp_dec_level2, out_enc_level2)\n",
    "\n",
    "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
    "\n",
    "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "\n",
    "        inp_dec_level1 = self.fuse1(inp_dec_level1, out_enc_level1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "\n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "\n",
    "        out_dec_level1 = self.output(out_dec_level1) + inp_img\n",
    "\n",
    "        return out_dec_level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "restormer_arch = Restormer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da53d5b",
   "metadata": {},
   "source": [
    "## Training  method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d78a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, ToPILImage, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import CenterCrop, Resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import *\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = clean_image_paths  #   \n",
    "        self.noisy_image_paths = noisy_image_paths  #   \n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((224, 224))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "        \n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return noisy_clean_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "            torch.manual_seed(seed)\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "            torch.manual_seed(seed)  # Re-seed to ensure identical transform\n",
    "            clean_image = self.transform(clean_image)\n",
    "        \n",
    "        return noisy_image, clean_image\n",
    "\n",
    "#            \n",
    "def create_dataloader(noisy_image_paths, clean_image_paths, batch_size, transform, test_size=0.2):\n",
    "    #    ID   \n",
    "    noisy_files_dict = {'_'.join(os.path.basename(x).split('_')[:-1]): x for x in noisy_image_paths}\n",
    "    clean_files_dict = {'_'.join(os.path.basename(x).split('_')[:-1]): x for x in clean_image_paths}\n",
    "    \n",
    "    # ID  noisy-clean  \n",
    "    noisy_clean_pairs = [(noisy_files_dict[id], clean_files_dict[id]) for id in noisy_files_dict if id in clean_files_dict]\n",
    "    if not noisy_clean_pairs:\n",
    "        print(\"No pairs found!\")\n",
    "        return None, None\n",
    "\n",
    "    noisy_files_matched, clean_files_matched = zip(*noisy_clean_pairs)\n",
    "\n",
    "    #   (8:2 )\n",
    "    train_noisy, val_noisy, train_clean, val_clean = train_test_split(\n",
    "        noisy_files_matched, clean_files_matched, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # CustomDataset      \n",
    "    train_dataset = CustomDataset(train_noisy, train_clean, transform=transform)\n",
    "    val_dataset = CustomDataset(val_noisy, val_clean, transform=transform)\n",
    "\n",
    "    #   \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader    \n",
    "    \n",
    "def train(model, optimizer, train_loader, val_loader, scheduler, scaler, device, patience=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.L1Loss().to(device) \n",
    "    \n",
    "    best_loss = float('inf')  #     \n",
    "    best_model = None\n",
    "    epochs_no_improve = 0  #   epoch   \n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        # Training loop\n",
    "        for noisy_imgs, clean_imgs in tqdm(iter(train_loader), desc=f\"Epoch {epoch}/{CFG['EPOCHS']}\"):\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #With autocast() gradscaler  float16       ,  .\n",
    "#             with autocast():\n",
    "            outputs = model(noisy_imgs)\n",
    "            l1_loss = criterion(outputs, clean_imgs)\n",
    "            \n",
    "#             scaler.scale(l1_loss).backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             scheduler.step()\n",
    "            l1_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss.append(l1_loss.item() * noisy_imgs.size(0))\n",
    "        \n",
    "        # Validation step\n",
    "        _val_loss = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss: [{_train_loss:.5f}] Val Loss: [{_val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if _val_loss < best_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "            epochs_no_improve = 0  #   \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy_imgs, clean_imgs in tqdm(iter(val_loader), desc=\"Validation\"):\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            pred = model(noisy_imgs)\n",
    "            \n",
    "            loss = criterion(pred, clean_imgs)\n",
    "            \n",
    "            val_loss.append(loss.item() * noisy_imgs.size(0))\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    return _val_loss\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for noisy_imgs in tqdm(test_loader, desc=\"Inference\"):\n",
    "            noisy_imgs = noisy_imgs.to(device).float()\n",
    "            pred = model(noisy_imgs)\n",
    "            \n",
    "            preds.append(pred.detach().cpu())\n",
    "    \n",
    "    preds = torch.cat(preds).numpy()\n",
    "\n",
    "    return preds\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_df(model, train_loader, val_loader):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG[\"LEARNING_RATE\"], weight_decay=1e-4)\n",
    "    criterion = nn.L1Loss()\n",
    "    scaler = GradScaler()\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=CFG[\"EPOCHS\"])\n",
    "\n",
    "    infer_model = train(model, optimizer, train_loader, val_loader, scheduler, scaler, device)\n",
    "    \n",
    "    return infer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960f7490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF, RF      : 3653\n",
      "RL, UD      : 3569\n",
      "LB, RB      : 3367\n",
      "      : 6829\n",
      "FF, RF   /  : 700 175\n",
      "RL, UD   /  : 680 171\n",
      "LB, RB   /  : 650 163\n",
      "   /  : 1303 326\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# \n",
    "base_dir = './data/Training'\n",
    "noisy_image_paths = os.path.join(base_dir, 'noisy')\n",
    "clean_image_paths = os.path.join(base_dir, 'clean')\n",
    "\n",
    "clean_files = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "\n",
    "#    \n",
    "ff_rf_paths = []\n",
    "rl_ud_paths = []\n",
    "lb_rb_paths = []\n",
    "other_blur_paths = []\n",
    "\n",
    "#    \n",
    "all_image_paths = glob(os.path.join(noisy_image_paths, '*.jpg'))\n",
    "\n",
    "#    \n",
    "for image_path in all_image_paths:\n",
    "    filename = os.path.basename(image_path)\n",
    "    \n",
    "    #    (FI )\n",
    "    blur_type = filename.split('_')[-1][:2]\n",
    "    \n",
    "    if blur_type in ['FF', 'RF']:\n",
    "        ff_rf_paths.append(image_path)\n",
    "    elif blur_type in ['RL', 'UD']:\n",
    "        rl_ud_paths.append(image_path)\n",
    "    elif blur_type in ['LB', 'RB']:\n",
    "        lb_rb_paths.append(image_path)\n",
    "    else:\n",
    "        other_blur_paths.append(image_path)\n",
    "\n",
    "#  \n",
    "print(\"FF, RF      :\", len(ff_rf_paths))\n",
    "print(\"RL, UD      :\", len(rl_ud_paths))\n",
    "print(\"LB, RB      :\", len(lb_rb_paths))\n",
    "print(\"      :\", len(other_blur_paths))\n",
    "\n",
    "#     \n",
    "train_transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "#     RandomRotation(degrees=10),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #official github    .\n",
    "])\n",
    "\n",
    "#  FF, RF, RL, UD,     \n",
    "ff_rf_train_loader, ff_rf_val_loader = create_dataloader(ff_rf_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "rl_ud_train_loader, rl_ud_val_loader = create_dataloader(rl_ud_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "lb_rb_train_loader, lb_rb_val_loader = create_dataloader(lb_rb_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "other_blur_train_loader, other_blur_val_loader = create_dataloader(other_blur_paths, clean_files, CFG[\"BATCH_SIZE\"], train_transform)\n",
    "\n",
    "#  \n",
    "print(\"FF, RF   /  :\", len(ff_rf_train_loader), len(ff_rf_val_loader))\n",
    "print(\"RL, UD   /  :\", len(rl_ud_train_loader), len(rl_ud_val_loader))\n",
    "print(\"LB, RB   /  :\", len(lb_rb_train_loader), len(lb_rb_val_loader))\n",
    "print(\"   /  :\", len(other_blur_train_loader), len(other_blur_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f67459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100% 650/650 [13:47<00:00,  1.27s/it]\n",
      "Validation: 100% 163/163 [00:59<00:00,  2.72it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss: [0.35467] Val Loss: [0.30619]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100% 650/650 [13:43<00:00,  1.27s/it]\n",
      "Validation: 100% 163/163 [00:59<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss: [0.29086] Val Loss: [0.28700]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100% 650/650 [13:46<00:00,  1.27s/it]\n",
      "Validation: 100% 163/163 [00:59<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss: [0.28780] Val Loss: [0.27334]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100% 650/650 [13:39<00:00,  1.26s/it]\n",
      "Validation: 100% 163/163 [00:59<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss: [154.20852] Val Loss: [0.79057]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100% 647/650 [13:33<00:03,  1.30s/it]"
     ]
    }
   ],
   "source": [
    "lb_rb_model = fftformer_arch.to(device)\n",
    "lb_rb_model.load_state_dict(torch.load('./fftformer_GoPro.pth')) #GoPro_pretrained \n",
    "\n",
    "lb_rb_model = train_df(lb_rb_model, lb_rb_train_loader, lb_rb_val_loader)\n",
    "#Epoch [5], Train Loss: [0.33701] Val Loss: [0.30922] autocast  o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lb_rb_model.state_dict(), 'lb_rb_fftformer_GoPro_1080.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_blur_model = fftformer_arch.to(device)\n",
    "other_blur_model.load_state_dict(torch.load('./fftformer_GoPro.pth')) #GoPro_pretrained \n",
    "\n",
    "# other_blur_model = Restormer().to(device)\n",
    "# other_blur_restormer_weight = torch.load('./single_image_defocus_deblurring.pth')\n",
    "# other_blur_model.load_state_dict(other_blur_restormer_weight['params'])\n",
    "\n",
    "other_blur_model = train_df(other_blur_model, other_blur_train_loader, other_blur_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cc202",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(other_blur_model.state_dict(), 'other_blur4_fftformer_GoPro_1080.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e213d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  52% 367/700 [03:07<02:43,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "ff_rf_model = restormer_arch.to(device)\n",
    "ff_rf_restormer_weight = torch.load('./single_image_defocus_deblurring.pth')\n",
    "ff_rf_model.load_state_dict(ff_rf_restormer_weight['params'])\n",
    "\n",
    "ff_rf_model = train_df(ff_rf_model, ff_rf_train_loader, ff_rf_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11af4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ff_rf_model.state_dict(), 'ff_rf_10epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFtformer     GPU \n",
    "rl_ud_model = fftformer_arch.to(device)\n",
    "rl_ud_model.load_state_dict(torch.load('./fftformer_GoPro.pth')) #GoPro_pretrained \n",
    "\n",
    "rl_ud_model = train_df(rl_ud_model, rl_ud_train_loader, rl_ud_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rl_ud_model.state_dict(), 'rl_ud_10epoch.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b7491",
   "metadata": {},
   "source": [
    "### Validation  \n",
    "psnr  ssim     (  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomDatasetVal(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = clean_image_paths#[os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "        self.noisy_image_paths = noisy_image_paths#[os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((224, 224))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "        \n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return noisy_clean_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "            torch.manual_seed(seed)\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "            torch.manual_seed(seed)  # Re-seed to ensure identical transform\n",
    "            clean_image = self.transform(clean_image)\n",
    "        \n",
    "        return noisy_image, clean_image\n",
    "    \n",
    "#     \n",
    "val_transform = Compose([\n",
    "#     RandomHorizontalFlip(),\n",
    "#     RandomVerticalFlip(),\n",
    "#     RandomRotation(degrees=10),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #official github    .\n",
    "])\n",
    "    \n",
    "# \n",
    "base_dir = './data/Validation'\n",
    "validation_noisy_image_paths = os.path.join(base_dir, 'noisy')\n",
    "validation_clean_image_paths = os.path.join(base_dir, 'clean')\n",
    "\n",
    "validation_clean_files = [os.path.join(validation_clean_image_paths, x) for x in os.listdir(validation_clean_image_paths)]\n",
    "\n",
    "#    \n",
    "validation_ff_rf_paths = []\n",
    "validation_rl_ud_paths = []\n",
    "validation_lb_rb_paths = []\n",
    "validation_other_blur_paths = []\n",
    "\n",
    "#    \n",
    "validation_all_image_paths = glob(os.path.join(validation_noisy_image_paths, '*.jpg'))\n",
    "\n",
    "#    \n",
    "for image_path in validation_all_image_paths:\n",
    "    filename = os.path.basename(image_path)\n",
    "    \n",
    "    #    (FI )\n",
    "    blur_type = filename.split('_')[-1][:2]\n",
    "    \n",
    "    if blur_type in ['FF', 'RF']:\n",
    "        validation_ff_rf_paths.append(image_path)\n",
    "    elif blur_type in ['RL', 'UD']:\n",
    "        validation_rl_ud_paths.append(image_path)\n",
    "    elif blur_type in ['LB', 'RB']:\n",
    "        validation_lb_rb_paths.append(image_path)\n",
    "    else:\n",
    "        validation_other_blur_paths.append(image_path)\n",
    "\n",
    "#  \n",
    "print(\"FF, RF      :\", len(validation_ff_rf_paths))\n",
    "print(\"RL, UD      :\", len(validation_rl_ud_paths))\n",
    "print(\"LB, RB      :\", len(validation_lb_rb_paths))\n",
    "print(\"      :\", len(validation_other_blur_paths))\n",
    "\n",
    "validation_ff_rf_dataset = CustomDatasetVal(validation_clean_files, validation_ff_rf_paths, transform=val_transform)\n",
    "validation_ff_rf_loader = DataLoader(validation_ff_rf_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "validation_rl_ud_dataset = CustomDatasetVal(validation_clean_files, validation_rl_ud_paths, transform=val_transform)\n",
    "validation_rl_ud_loader = DataLoader(validation_rl_ud_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "validation_lb_rb_dataset = CustomDatasetVal(validation_clean_files, validation_lb_rb_paths, transform=val_transform)\n",
    "validation_lb_rb_loader = DataLoader(validation_lb_rb_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "validation_other_blur_dataset = CustomDatasetVal(validation_clean_files, validation_other_blur_paths, transform=val_transform)\n",
    "validation_other_blur_loader = DataLoader(validation_other_blur_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "    \n",
    "ff_rf_model = restormer_arch.to(device)\n",
    "ff_rf_model.load_state_dict(torch.load('./ff_rf_10epoch.pth'))\n",
    "rl_ud_model = fftformer_arch.to(device)\n",
    "rl_ud_model.load_state_dict(torch.load('./rl_ud_10epoch.pth'))\n",
    "lb_rb_model = fftformer_arch.to(device)\n",
    "lb_rb_model.load_state_dict(torch.load('./lb_rb_fftformer_GoPro_1080'))\n",
    "other_blur_model = fftformer_arch.to(device)\n",
    "other_blur_model.load_state_dict(torch.load('./other_blur_fftformer.pth'))\n",
    "    \n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_value = 1. if img1.max() <= 1 else 255.\n",
    "    psnr = 20 * torch.log10(max_value / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"Calculate SSIM (Structural Similarity Index) between two images.\"\"\"\n",
    "    # Convert tensors to numpy arrays and remove extra batch dimension\n",
    "    img1_np = img1.permute(1, 2, 0).cpu().numpy()\n",
    "    img2_np = img2.permute(1, 2, 0).cpu().numpy()\n",
    "    # Calculate SSIM between the two images\n",
    "    ssim_value = ssim(img1_np, img2_np, multichannel=True, data_range=img2_np.max() - img2_np.min())\n",
    "    return ssim_value\n",
    "\n",
    "def validate_metrics(ff_rf_model, rl_ud_model, lb_rb_model, other_blur_model, validation_ff_rf_loader, validation_rl_ud_loader, validation_lb_rb_loader, validation_other_blur_loader, device):\n",
    "    ff_rf_model.eval()\n",
    "    rl_ud_model.eval()\n",
    "    lb_rb_model.eval()\n",
    "    other_blur_model.eval()\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for noisy_imgs, clean_imgs in validation_ff_rf_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            pred_imgs = ff_rf_model(noisy_imgs)\n",
    "            \n",
    "            for i in range(len(pred_imgs)):\n",
    "                psnr_value = calculate_psnr(pred_imgs[i], clean_imgs[i])\n",
    "                ssim_value = calculate_ssim(pred_imgs[i], clean_imgs[i])\n",
    "                \n",
    "                psnr_values.append(psnr_value)\n",
    "                ssim_values.append(ssim_value)\n",
    "                \n",
    "                \n",
    "        for noisy_imgs, clean_imgs in validation_rl_ud_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            pred_imgs = rl_ud_model(noisy_imgs)\n",
    "            \n",
    "            for i in range(len(pred_imgs)):                \n",
    "                psnr_value = calculate_psnr(pred_imgs[i], clean_imgs[i])\n",
    "                ssim_value = calculate_ssim(pred_imgs[i], clean_imgs[i])\n",
    "                \n",
    "                psnr_values.append(psnr_value)\n",
    "                ssim_values.append(ssim_value)\n",
    "        \n",
    "        for noisy_imgs, clean_imgs in validation_lb_rb_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            pred_imgs = lb_rb_model(noisy_imgs)\n",
    "            \n",
    "            for i in range(len(pred_imgs)):                \n",
    "                psnr_value = calculate_psnr(pred_imgs[i], clean_imgs[i])\n",
    "                ssim_value = calculate_ssim(pred_imgs[i], clean_imgs[i])\n",
    "                \n",
    "                psnr_values.append(psnr_value)\n",
    "                ssim_values.append(ssim_value)\n",
    "                \n",
    "        for noisy_imgs, clean_imgs in validation_other_blur_loader:\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "            \n",
    "            pred_imgs = other_blur_model(noisy_imgs)\n",
    "            \n",
    "            for i in range(len(pred_imgs)):                \n",
    "                psnr_value = calculate_psnr(pred_imgs[i], clean_imgs[i])\n",
    "                ssim_value = calculate_ssim(pred_imgs[i], clean_imgs[i])\n",
    "                \n",
    "                psnr_values.append(psnr_value)\n",
    "                ssim_values.append(ssim_value)\n",
    "\n",
    "    avg_psnr = np.mean(psnr_values)\n",
    "    avg_ssim = np.mean(ssim_values)\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    return avg_psnr, avg_ssim\n",
    "\n",
    "# Visualize predictions and calculate PSNR\n",
    "avg_psnr, avg_ssim = validate_metrics(ff_rf_model, rl_ud_model, other_blur_model, validation_ff_rf_loader, validation_rl_ud_loader, validation_other_blur_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######  () ######\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, splitext\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from PIL import Image\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "#  \n",
    "noisy_data_path = './open/test/input'\n",
    "output_path = './open/test/submission'\n",
    "\n",
    "#    \n",
    "test_ff_rf_paths = []\n",
    "test_rl_ud_paths = []\n",
    "test_lb_rb_paths = []\n",
    "test_other_blur_paths = []\n",
    "\n",
    "#    \n",
    "test_all_image_paths = glob(os.path.join(noisy_data_path, '*.jpg'))\n",
    "\n",
    "#    \n",
    "for image_path in test_all_image_paths:\n",
    "    filename = os.path.basename(image_path)\n",
    "    \n",
    "    #    (FI )\n",
    "    blur_type = filename.split('_')[-1][:2]\n",
    "    \n",
    "    if blur_type in ['FF', 'RF']:\n",
    "        test_ff_rf_paths.append(image_path)\n",
    "    elif blur_type in ['RL', 'UD']:\n",
    "        test_rl_ud_paths.append(image_path)\n",
    "    elif blur_type in ['LB', 'RB']:\n",
    "        test_lb_rb_paths.append(image_path)\n",
    "    else:\n",
    "        test_other_blur_paths.append(image_path)\n",
    "\n",
    "#  \n",
    "print(\"FF, RF      :\", len(test_ff_rf_paths))\n",
    "print(\"RL, UD      :\", len(test_rl_ud_paths))\n",
    "print(\"LB, RB      :\", len(test_lb_rb_paths))\n",
    "print(\"      :\", len(test_other_blur_paths))\n",
    "\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, transform=None):\n",
    "        self.noisy_image_paths = noisy_image_paths #[join(noisy_image_paths, x) for x in listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        \n",
    "        # Convert numpy array to PIL image\n",
    "        if isinstance(noisy_image, np.ndarray):\n",
    "            noisy_image = Image.fromarray(noisy_image)\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path\n",
    "\n",
    "test_transform = Compose([\n",
    "    ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    #Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) official github    .\n",
    "])\n",
    "\n",
    "test_ff_rf_dataset = CustomDatasetTest(test_ff_rf_paths, transform=test_transform)\n",
    "test_ff_rf_loader = DataLoader(test_ff_rf_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "test_rl_ud_dataset = CustomDatasetTest(test_rl_ud_paths, transform=test_transform)\n",
    "test_rl_ud_loader = DataLoader(test_rl_ud_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "test_lb_rb_dataset = CustomDatasetTest(test_lb_rb_paths, transform=test_transform)\n",
    "test_lb_rb_loader = DataLoader(test_lb_rb_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)   \n",
    "test_other_blur_dataset = CustomDatasetTest(test_other_blur_paths, transform=test_transform)\n",
    "test_other_blur_loader = DataLoader(test_other_blur_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False)    \n",
    "    \n",
    "ff_rf_model = restormer_arch.to(device)\n",
    "ff_rf_model.load_state_dict(torch.load('./ff_rf_10epoch.pth'))\n",
    "rl_ud_model = fftformer_arch.to(device)\n",
    "rl_ud_model.load_state_dict(torch.load('./rl_ud_10epoch.pth'))\n",
    "lb_rb_model = fftformer_arch.to(device)\n",
    "lb_rb_model.load_state_dict(torch.load('./lb_rb_fftformer_GoPro_1080'))\n",
    "other_blur_model = fftformer_arch.to(device)\n",
    "other_blur_model.load_state_dict(torch.load('./other_blur_fftformer.pth'))\n",
    "\n",
    "#  denoising  \n",
    "for noisy_image, noisy_image_path in test_ff_rf_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = ff_rf_model(noisy_image)\n",
    "    \n",
    "    # denoised_image CPU   \n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = torch.clamp(denoised_image, 0, 1)  #   0 1  \n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.png'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')\n",
    "\n",
    "for noisy_image, noisy_image_path in test_rl_ud_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = rl_ud_model(noisy_image)\n",
    "    \n",
    "    # denoised_image CPU   \n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = torch.clamp(denoised_image, 0, 1)  #   0 1  \n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.png'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')\n",
    "\n",
    "for noisy_image, noisy_image_path in test_lb_rb_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = lb_rb_model(noisy_image)\n",
    "    \n",
    "    # denoised_image CPU   \n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = torch.clamp(denoised_image, 0, 1)  #   0 1  \n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.png'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')\n",
    "\n",
    "for noisy_image, noisy_image_path in test_other_blur_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = other_blur_model(noisy_image)\n",
    "    \n",
    "    # denoised_image CPU   \n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = torch.clamp(denoised_image, 0, 1)  #   0 1  \n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.jpg'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_folder(folder_path, output_zip):\n",
    "    shutil.make_archive(output_zip, 'zip', folder_path)\n",
    "    print(f\"Created {output_zip}.zip successfully.\")\n",
    "\n",
    "zip_folder(output_path, './submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
